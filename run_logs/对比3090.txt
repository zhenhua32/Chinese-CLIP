accum_freq: 1
aggregate: True
batch_size: 400
bert_weight_path: None
beta1: 0.9
beta2: 0.98
checkpoint_path: ../datapath//experiments/muge_finetune_vit-b-16_roberta-base_bs48_1gpu/checkpoints
clip_weight_path: None
context_length: 52
debug: False
device: cuda:0
eps: 1e-06
freeze_vision: False
gather_with_grad: False
grad_checkpointing: True
local_device_rank: 0
log_interval: 10
log_level: 20
log_path: ../datapath//experiments/muge_finetune_vit-b-16_roberta-base_bs48_1gpu/out_2023-05-28-04-00-34.log
logs: ../datapath//experiments/
lr: 3e-06
mask_ratio: 0
max_epochs: 1
max_steps: 626
name: muge_finetune_vit-b-16_roberta-base_bs48_1gpu
num_workers: 4
precision: amp
rank: 0
report_training_batch_acc: True
reset_data_offset: True
reset_optimizer: True
resume: ../datapath//pretrained_weights/clip_cn_vit-b-16.pt
save_epoch_frequency: 1
save_step_frequency: 999999
seed: 123
skip_aggregate: False
skip_scheduler: False
text_model: RoBERTa-wwm-ext-base-chinese
train_data: ../datapath//datasets/MUGE/lmdb/train
use_augment: True
use_bn_sync: False
use_flash_attention: False
val_data: ../datapath//datasets/MUGE/lmdb/valid
valid_batch_size: 48
valid_epoch_interval: 1
valid_num_workers: 4
valid_step_interval: 1000
vision_model: ViT-B-16
warmup: 100
wd: 0.001
world_size: 1
