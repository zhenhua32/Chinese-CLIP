accum_freq: 1
aggregate: True
batch_size: 128
bert_weight_path: None
beta1: 0.9
beta2: 0.98
checkpoint_path: ./datapath/experiments/muge_finetune_vit-b-16_roberta-base_bs128_1gpu\checkpoints
clip_weight_path: None
context_length: 52
debug: False
device: cuda:0
eps: 1e-06
freeze_vision: False
gather_with_grad: False
grad_checkpointing: False
local_device_rank: 0
log_interval: 1
log_level: 20
log_path: ./datapath/experiments/muge_finetune_vit-b-16_roberta-base_bs128_1gpu\out_2023-05-15-14-57-43.log
logs: ./datapath/experiments/
lr: 5e-05
mask_ratio: 0
max_epochs: 3
max_steps: 5868
name: muge_finetune_vit-b-16_roberta-base_bs128_1gpu
num_workers: 4
precision: amp
rank: 0
report_training_batch_acc: True
reset_data_offset: True
reset_optimizer: True
resume: ./datapath/pretrained_weights/clip_cn_vit-b-16.pt
save_epoch_frequency: 1
save_step_frequency: 999999
seed: 123
skip_aggregate: False
skip_scheduler: False
text_model: RoBERTa-wwm-ext-base-chinese
train_data: ./datapath/datasets/MUGE/lmdb/train
use_augment: True
use_bn_sync: False
use_flash_attention: False
val_data: ./datapath/datasets/MUGE/lmdb/valid
valid_batch_size: 128
valid_epoch_interval: 1
valid_num_workers: 1
valid_step_interval: 150
vision_model: ViT-B-16
warmup: 100
wd: 0.001
world_size: 1
